### Noted Issues and Accompanying Corrections

This file contains a set of corrections for the previously submitted thesis. See below.

* Poorly written abstract in an attempt to save word count.
    * Fix: Replace the abstract with the original version, see [abstract.tex](corrections/thesis/abstract.tex).
* Inconsistent (or undefined) notation, e.g. using $Ret_X(q)_k$ when the $top-k$ prefix is undefined.
    * Fix: Add "In instances where only the top-k candidates are selected, the ranked list is written $Ret_X(q)_{1:k}$ for the top-k prefix." under Problem Definition ยง3.1.
* Introduction of free variables that ought to be bound, e.g. nDCG should use $dist(C^\star, D)$, not $dist(C,D)$, as C is unbound.
    * Fix: Replace $dist(C,D)$ with $dist(C^\star, D)$, under ยง4.5.
* Documented evaluation metric (nDCG) provides good intuition, but does not fully reflect implementation (one-to-one); not a huge issue as it would technically yield the same results, but principally, should be addressed (the footnote does at least draw attention to this).
* nDCG also defines $dist(C^\star, D) = 0 \iff C^\star = D$; however, this is misleading (and technically inaccurate), since the quotient poset detailed under ยง3.1. means that any equivalence relations are unaccounted for in this description.
    * Fix: Replace with $dist(C^\star, D) = 0 \iff C^\star \equiv D$; and we might **explicitly** note that this does, in fact, induce a partial-order (a requisite for the subsequent transitive reduction and resulting Hasse Diagram; the way in which it is currently written under ยง3.1. implies this, though **it really should be made explicit**; and this, likely, belongs within a *larger proof*). The *(Work In Progress)* Proof [is available here.](./proofs/polyhierarchy.md)
* HiT dataset construction **likely** requires the use of a reasoner as part of the build process, i.e. passing `elk` as an argument for `reasoner_type` when loading the ontology prior to instanciating the `HierarchyDatasetConstructor` (see `./scripts/load_taxonomy.py`, line 57-58). The characterisation of "using the stated view" to train the encoders *(as written in the thesis)* is accurate in the case of OnT, since reasoning is an implicit operation during ELNormalisation, i.e $\sqsubseteq_{st}^{\ast} \rightarrow$ gets atomic concepts $\rightarrow$ gets role dependencies $\rightarrow$ applies EL normalisation $\rightarrow$ reflexive transitive closure $\rightarrow$ verbalisation. However, the build scripts as originally shipped set `reasoner_type` to `struct` when building the HiT dataset for training.
    * Fix: Modify the `./scripts/build_hit_data.sh` line 30 to line 31, and add `--infer` (which tells the HiT Dataset Constructor to use `elk` rather than `struct`) and *perhaps* add a footnote to the thesis *(it should likely be documented somewhere, it took me a while to figure this out)*.
* The shipped evaluation dataset appears to contain **some** instances where $\equiv$ classes had not properly been inferred prior to constructing their ancestor sets, resulting in a shallow hierarchy for a minority of cases. However, the build scripts that were shipped do function as intended, and as such the dataset can be accurately re-constructed using the provided scripts.
    * Fix: See the [updated evaluation dataset](./data/evaluation_dataset.json) in [the data directory.](./data).
* The pipeline for OnT-training that has been shipped via `Makefile` (`make ont-data`) fails to apply normalisation to the class labels or role labels (which is required when training on SNOMED~CT due to leakage). Note that the process was undertaken originally during model training, however, it has been failed to be included as part of the full build pipeline *(it will train, it just won't remove high-level semantic branches from verbalisations prior to training)*.
    * Fix: Include [this (`preprocess_ont_dir.py`)](./scripts/preprocess_ont_dir.py) script during the build pipeline after having run `ELNormalizeData.py` with the following parameters `python ./scripts/preprocess_ont_dir.py --base-dir ./data/ont_dataset --strip-parentheses --to-lower-case --collapse-whitespace`.
    * Fix: Modify `./scripts/build_ont_data.sh` to include `conda run -n "$AUTO_ENV_NAME" python ./scripts/preprocess_ont_dir.py --base-dir ./data/ont_dataset --strip-parentheses --to-lower-case --collapse-whitespace` after `ELNormalizeData.py` completes.
* The discussion of the HiT/OnT training procedure is inaccurate. HiT/OnT training struggles with $NF1$, not $NF2 \rightarrow NF4$, in fact $NF2 \rightarrow NF4$ performs well, $NF1$ is where the models struggle *(this mistake was likely made by accidently misreading results as reported directly within the console)*.
    * Fix: Correct the `retrieval-notebook.ipynb` and Appendix.F discussion of assumption testing regarding ontology size to reflect the correctly interpreted results *(note that this doesn't change the conslusion, it is simply a mischaracterisation)*.